Training/evaluation parameters %s Namespace(adam_epsilon=1e-08, cache_dir='', config_name='', data_dir='../GLUE/MRPC', device=device(type='cuda'), do_eval=True, do_lower_case=True, do_train=True, eval_all_checkpoints=True, evaluate_during_training=False, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, learning_rate=2e-05, local_rank=-1, logging_steps=50, max_grad_norm=1.0, max_seq_length=128, max_steps=-1, model_name_or_path='kssteven/ibert-roberta-base', model_type='ibert-roberta', n_gpu=1, no_cuda=False, num_train_epochs=1.0, output_dir='./saved_models/ibert-roberta-base/MRPC/raw', output_mode='classification', overwrite_cache=True, overwrite_output_dir=True, per_gpu_eval_batch_size=1, per_gpu_train_batch_size=8, save_steps=0, seed=42, server_ip='', server_port='', task_name='mrpc', tokenizer_name='', warmup_steps=0, weight_decay=0.0)
Creating features from dataset file at %s ../GLUE/MRPC
Saving features into cached file %s ../GLUE/MRPC/cached_train_ibert-roberta-base_128_mrpc
***** Running training *****
  Num examples = %d 3668
  Num Epochs = %d 1.0
  Instantaneous batch size per GPU = %d 8
  Total train batch size (w. parallel, distributed & accumulation) = %d 8
  Gradient Accumulation steps = %d 1
  Total optimization steps = %d 459.0
 global_step = %s, average loss = %s 459 0.5057281213761804
Saving model checkpoint to %s ./saved_models/ibert-roberta-base/MRPC/raw
Evaluate the following checkpoints: %s ['./saved_models/ibert-roberta-base/MRPC/raw']
Creating features from dataset file at %s ../GLUE/MRPC
Saving features into cached file %s ../GLUE/MRPC/cached_dev_ibert-roberta-base_128_mrpc
***** Running evaluation  *****
  Num examples = %d 408
  Batch size = %d 1
Eval time: 8.211565017700195
***** Eval results  *****
  %s = %s acc 0.8602941176470589
  %s = %s acc_and_f1 0.8793449197860963
  %s = %s f1 0.8983957219251337
